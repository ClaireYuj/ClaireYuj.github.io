---
title: 机器学习3-多元梯度下降
top: false
cover: false
author: DULULU oO
date: 2021-07-30 16:32:09
mathjax: true
password:
summary:
tags:
    - 梯度下降
categories: 机器学习
---

## 多元线性回归

多元线性回归也就是用多个特征变量来进行预测  

<div align = 'center'><font size =4>$h_\theta$($\theta$) = $\theta_0$+$\theta_1$$x_1$+$\theta_2$$x_2$+$\theta_3$$x_3$+$\theta_4$$x_4$...+$\theta_n$$x_n$</font></div>  
  

<center>定义$x_0$ = 1</center>  

所以
    x = 
        $$\begin{bmatrix}
        x_0 x_1 x_2 ... x_n
        \end{bmatrix}$$

$\theta$ = $$\begin{bmatrix}\theta_0 \\ \theta_1  \\ \theta_2 \\ ... \theta_n\end{bmatrix}$$

所以得出$h_\theta(x) = \theta^Tx$


## 用梯度下降处理多元线性回归

在梯度下降中的假设
$$h_\theta(x) = \theta^Tx$$

代价函数

$$J(\theta_0,\theta_1...\theta_n) = \frac{1}{2m} \displaystyle \sum_{i=1}^m (h_\theta(x^{(i)}-y^{(i)}))^2$$

梯度下降
<center>重复多次</center>
    
$$\theta := \theta_j - \alpha \frac{\partial {J(\theta)}}{\partial \theta_j}$$

$\alpha$是学习率

最终梯度下降
<center>重复</center>
$$ \theta_j := \theta_j - \alpha \frac{1}{m} \displaystyle \sum_{i=1}^m (h_\theta(x^{(i)}-y^{(i)}))x_j^{(i)}$$

即
$$ \theta_0 := \theta_0 - \alpha \frac{1}{m} \displaystyle \sum_{i=1}^m (h_\theta(x^{(i)}-y^{(i)}))x_0^{(i)}$$
$$ \theta_1 := \theta_1 - \alpha \frac{1}{m} \displaystyle \sum_{i=1}^m (h_\theta(x^{(i)}-y^{(i)}))x_1^{(i)}$$
$$ \theta_2 := \theta_2 - \alpha \frac{1}{m} \displaystyle \sum_{i=1}^m (h_\theta(x^{(i)}-y^{(i)}))x_2^{(i)}$$


## 特征缩放(Feature scaling)

> 不同特征的取值在一个相近的范围，这样梯度下降容易更好地收敛

特征地范围不要过小也不要过大

### 均值归一化(Mean normalization)

> 将$x_i$用$x_i-\mu_i$代替是的特征均值为0

