---
title: 机器学习1--机器学习是干什么
top: false
cover: false
author: DULULU oO
date: 2021-07-20 11:21:56
password:
summary:
tags:
    - 梯度下降
    - 代价函数
categories: 机器学习
---

监督学习与非监督学习，假设函数与代价函数，梯度下降

## 监督学习与非监督学习

> **有无预期输出**是监督学习与非监督学习的最大区别


- 监督学习：学习一个模型使模型能够对任意给定的输入，对其相应的输出做出一个好的预测。

即：利用训练数据集学习一个模型，再用模型对测试样本集进行预测。

**分类问题（离散）**与**回归问题（连续）**等都是监督学习。

- 非监督学习：直接对数据进行建模。没有给定事先标记过的训练范例，所用的数据没有属性或标签这一概念。事先不知道输入数据对应的输出结果是什么。

自动对输入的资料进行分类或分群，以寻找数据的模型和规律。

**聚类算法**是一种非监督学习，针对数据集，自动找出数据中的结构，从而把数据分成不同的簇。

## 代价函数

此处以最简单的线性回归（如房价预测）为例，线性回归的学习是为了预测未来房价

机器学习是在已知部分x与y的情况下，得出一个房价相关的线性函数y = ax + b

所以代价函数是与参数a,b相关的函数,使得代价函数最小化，是线性回归中的学习目标

常用的代价函数有
    ![平方和代价函数](/img/posts/MachineLearning/cost_function.jpg)


## 梯度下降

### 梯度下降的过程

1. 选定几个参数a1,a2,a3...
2. 选择初始值
3. 从初值开始按照给定的步幅进行梯度下降，寻找局部极小值

![梯度下降可以用等高线图具象化](/img/posts/MachineLearning/slope0.jpg)

![选择初始点](/img/posts/MachineLearning/slope1.jpg)

![从a点一步一步往下梯度下降](/img/posts/MachineLearning/slope3.jpg)

![从b点一步一步往下梯度下降](/img/posts/MachineLearning/slope2.jpg)

> 从不同的点进行梯度下降可能得到**不同**的局部极小值

### 梯度下降的数理解释

![梯度下降的数理解释](/img/posts/MachineLearning/slope_definition.jpg)

a是学习率
下一步的值 = 这一步的值 - 学习率*代价函数分别对参数a1,a2,a3...的偏导

对于梯度下降公式中偏导的简化

![符合求偏导](/img/posts/MachineLearning/slope_deri0.jpg)

梯度下降是一种**同步更新**算法，要求参数a1,a2,a3同步更新，所以
![要按照左侧的顺序来](/img/posts/MachineLearning/slope_detail0.jpg)

### 线性回归中的梯度下降


![梯度下降再线性回归中的模型](/img/posts/MachineLearning/slope_linear.jpg)

将代数函数用等高线图（右侧）表示

![由于该代数函数是一个凹函数，所以局部极小就是全局最小，因此要得到左侧最拟合的目标函数，需要右侧趋近于等高线图最中间的圆圈](/img/posts/MachineLearning/slope4.jpg)