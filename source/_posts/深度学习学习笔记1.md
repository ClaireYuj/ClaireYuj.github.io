---
title: 深度学习学习笔记1
top: true
cover: false
author: DULULU oO
date: 2021-08-05 11:05:55
mathjax: true
password:
summary:
tags: 
    - 深度学习
    - 机器学习
categories: 机器学习
---



## 关于深度学习


深度学习是通过多层来获得更为抽象的特征表达，如果有一个n层(S1,S2,S3...Sn)的深度学习系统, 输入为I，输出是O，那么深度学习的过程可以抽象的表示为: I =>S1=>S2=>S3=>...=>O，每一层的信息处理都会丢失部分信息，最后的O很难是初始的I，**但是如果我们可以通过深度学习调节系统中的参数使得它的输出仍然是I那么我们就可以自动的获取输入I的一系列层次特征，即S1，S2...Sn**

![神经网络与深度学习结构(图片选自《神经网络与深度学习》一邱锡鹏](/img/posts/MachineLearning/Mindmap.jpg)

[参考学习](https://blog.csdn.net/qq_36816848/article/details/122286610?ops_request_misc=%257B%2522request%255Fid%2522%253A%2522166244950816782391845436%2522%252C%2522scm%2522%253A%252220140713.130102334..%2522%257D&request_id=166244950816782391845436&biz_id=0&utm_medium=distribute.pc_search_result.none-task-blog-2~all~top_click~default-2-122286610-null-null.142^v46^pc_rank_34_2&utm_term=%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0&spm=1018.2226.3001.4187)


### 浅层学习

是只含有一层隐层节点/没有节点的浅层模型，如支持向量机、最大熵方法（逻辑回归）、多层感知机等

![感知机perceptron](/img/posts/MachineLearning/perceptron.jpg)

![简单感知机](/img/posts/MachineLearning/simple_perceptron.jpg)

![多层感知机](/img/posts/MachineLearning/multi_layel_perceptron.jpg)

利用人工神经网络的反向传播算法（Back Propagation算法或者BP算法），让一个人工神经网络模型从大量训练样本中学习统计规律，从而对未知事件做预测。

- 前向传播与反向传播：前向传播（foward propagation）作用于**每一层的输入**，通过逐层计算得到输出结果；反向传播（backward propagation）作用于网络的**输出**，通过计算**梯度**由**深**到**浅**更新网络参数。

由于我们前向传播最终得到的结果，以分类为例，最终总是有误差的，可以通过**梯度下降算法**减少误差。

### 深度学习

深度学习的实质，是通过构建具有很多隐层的机器学习模型和海量的训练数据，来学习更有用的特征，从而最终提升分类或预测的准确性。**通过深度学习最终达到特征学习的目标**

在深度学习中，明确突出了特征学习的重要性，通过**逐层特征变换**，将样本在原空间的特征表示变换到一个新特征空间，从而使分类或预测更加容易。

## 深度学习与神经网络

![神经网络的结构](/img/posts/MachineLearning/neutral_network.jpg)

![神经网络的主要模型](/img/posts/MachineLearning/model_nw.jpg)

深度学习采用了神经网络相似的分层结构，系统由包括输入层、隐层（多层）、输出层组成的多层网络，只有相邻层节点之间有连接，同一层以及跨层节点之间相互无连接，每一层可以看作是一个**逻辑回归**模型。

但DL与传统神经网络的区别在于，传统神经网络中，采用的是反向传播的方式进行，也就是采用迭代的算法来训练整个网络，随机设定初值，计算当前网络的输出，然后根据当前输出和label之间的差去改变前面各层的参数，直到收敛（整体是一个梯度下降法）。而DL整体上是一个layer-wise的训练机制。这样做的原因是因为，如果采用b反向传播的机制，对于一个深层网络（7层以上），残差传播到最前面的层已经变得太小，便会出现梯度扩散(gradient diffusion)。因此传统的BP算法不能用在深度神经网络。

- BP算法用于深度学习会出现的问题：
1. 梯度越来越稀疏，偏差原来越小，可能出现过拟合
2. 由于随机初始值，最终收敛到局部最小
3. 只能通过有标签的数据集进行训练，不能学习无标签的数据集，也就是监督学习

### 训练过程

[参考博客](https://blog.csdn.net/zouxy09/article/details/8775518)

如果对所有层同时训练，时间复杂度会太高；如果每次训练一层，偏差就会逐层传递。这会面临跟上面监督学习中相反的问题，会严重欠拟合（因为深度网络的神经元和参数太多了）。

简单来说分为两步：

1）首先逐层构建单层神经元，这样每次都是训练一个单层网络。

2）当所有层训练完后，使用wake-sleep算法进行调优。

Wake-Sleep算法分为醒（wake）和睡（sleep）两个部分。

1）wake阶段：认知过程，通过外界的特征和向上的权重（认知权重）产生每一层的抽象表示（结点状态），并且使用梯度下降修改层间的下行权重（生成权重）。也就是“如果现实跟我想象的不一样，改变我的权重使得我想象的东西就是这样的”。

2）sleep阶段：生成过程，通过顶层表示（醒时学得的概念）和向下权重，生成底层的状态，同时修改层间向上的权重。也就是“如果梦中的景象不是我脑中的相应概念，改变我的认知权重使得这种景象在我看来就是这个概念”。

#### 使用自下而上的非监督学习

采用无标定数据（有标定数据也可）分层训练各层参数（可以看作无监督训练过程--也是特征学习的过程）

1. 首先用无标定数据训练第一层，训练时先学习第一层的参数，可以将这一步看作一个使得输出如输出差异化的最小神经网络的隐层
2. 一次学习没在学习到低n层时，将低n层的输出作为第n+1层的输入来训练第n+1层，以此得到每一层的参数

#### 采用自顶而下的监督学习

通过有标签的数据去训练，将误差自顶而下传输，堆网络进行微调

基于第一步各层参数进一步微调整个模型的参数，由于DL的第一步不是随机初始化，而是通过学习输入数据的结构得到的，因而这个初值更接近全局最优，从而能够取得更好的效果；所以DL效果好很大程度上归功于第一步自下而上非监督学习中的特征学习过程。

## 超参数

超参数是在**开始学习过程之前设置值的参数**，而不是通过训练得到的参数数据。通常情况下，需要对超参数进行优化，给学习机选择一组最优超参数，以提高学习的性能和效果。具体来讲比如算法中的学习率、梯度下降法迭代的数量、隐藏层数目、隐藏层单元数目、激活函数都需要根据实际情况来设置，这些参数实际上控制了最后的参数和的值，所以它们被称作超参数。

因此，寻求超参数的最优值是十分重要的，常见的设置超参数的方法有

1. 随机搜索：让计算机随机挑选以阻止
2. 贝叶斯优化：使用贝叶斯优化超参数，但贝叶斯算法本身就有很多困难。
3. MITIE方法：先进性局部优化，利于寻找局部最优解。

**超参数搜索的一般过程是**
1. 将数据集划分成训练集、验证集及测试集。
2. 在训练集上根据模型的性能指标对模型参数进行优化。
3. 在验证集上根据模型的性能指标对模型的超参数进行搜索
4. 步骤 2 和步骤 3 交替迭代，最终确定模型的参数和超参数，在测试集中验证评价模型的优劣

## 激活函数 

激活函数将非线性特性引入到网络中，在神经网络的神经元中，输入的 inputs 通过加权，求和后，还被作用了一个函数，这个函数就是激活函数。激活函数是为了增加神经网络模型的**非线性**。**没有激活函数的每层都相当于矩阵相乘。就算你叠加了若干层之后，无非还是个矩阵相乘罢了。**激活函数还可以把当前特征空间通过一定的线性映射转换到另一个空间，让数据能够更好的被分类。

- 激活函数是个非线性函数：
    1. 假若网络中全部是线性部件，那么线性的组合还是线性，与单独一个线性分类器无异。这样就做不到用非线性来逼近任意函数
    2. 使用非线性激活函数 ，以便使网络更加强大，增加它的能力，使它可以学习复杂的事物，复杂的表单数据，以及表示输入输出之间非线性的复杂的任意函数映射。使用非线性激活函数，能够从输入输出之间生成非线性映射。
    
常见的激活函数有：Logistic函数，Tanh函数，ReLU函数，SoftPlus函数，可见参考[【推荐】深度学习笔记——常用的激活（激励）函数](https://www.cnblogs.com/wj-1314/p/12015278.html)
![激活函数](/img/posts/MachineLearning/activate_fun.jpg)

### Sigmoid
  
Sigmoid也就是Logistic函数，表达式如下：  

$${S(x)} = \frac{1}{1+e^{(-x)}}$$  

![Sigmoid函数即其导数图像](/img/posts/MachineLearning/Sigmoid.jpg)
绘图对应代码：
```Python
from matplotlib import pyplot as plt
import numpy as np
fig = plt.figure()
# np.arrange(起点，终点，步长)
x = np.arange(-10, 10, 0.025) 
# plot(x,y)
plt.plot(x,1/(1+np.exp(-x)))
plt.title("y = 1/(1+exp(-x))")
# savefig存储要在show之前，不然输出的图片 会是空白
plt.savefig('img/output_relu\'(x).png')
plt.show()
 
plt.plot(x,np.exp(-x)/(1+np.exp(-x))**2)
plt.title("y = exp(-x)/(1+exp(-x))^2")
plt.show()

```
- Sigmoid函数连续可导，值域为(0,1)
- 相比较于阶跃函数，可以直接利用梯度下降算法优化网络参数
- 作为激活函数，将inputs映射到(0,1)，在早期的神经网络中使用地非常多，因为它很好地解释了神经元受到刺激后是否被激活和向后传递的场景（0：几乎没有被激活；1：完全被激活）。因此可以通过Simoid函数将**输出转化为概率输出**，常用于分类问题的**事件概率**

$$ {S'(x)} = {S(x)}{(1-S(x))} = \frac{e^{-x}}{(1+e^{-x})}$$

- 在input x = 0时，导数醉倒为0.25，当输入为正负无穷时，导数趋于0，会发生梯度弥散  
- Sigmoid函数光滑，易于求导，但是时指数级计算，计算量大，容易出现梯度弥散或者梯度饱和
- 当神经网络的层数很多时，如果每一层的激活函数都采用Sigmoid函数的话，就会产生梯度弥散和梯度爆炸的问题，其中梯度爆炸发生的概率非常小，而梯度消失发生的概率比较大。

当一个激活函数S(x)满足
右饱和：$ \lim_{n\to +\infty} {S(x)} = 0 $
左饱和：$ \lim_{n\to -\infty} {S(x)} = 0 $
当S(x)同时满足左饱和与右饱和时，称之为饱和

### tanh

tanh是双曲函数中的一个，tanh() 为双曲正切，关于原点中心对称。在数学中，双曲正切 tanh 是由双曲正弦和双曲余弦这两者基本双曲函数推导而来。  

正切函数时非常常见的激活函数，**与Sigmoid函数相比，它的输出均值是0**，使得其**收敛速度要比Sigmoid快**，减少迭代次数。相对于Sigmoid的好处是它的输出的均值为0，克服了第二点缺点。**但是当饱和的时候还是会杀死梯度。**

$$ {tanh(x)} = \frac {1-e^{-2x}}{1+e^{-2x}}$$

$$ {tanh'(x)} = {1-(tanh(x)^2)} =\frac {4e^{-2x}}{(1+e^{-2x})^2}$$

![tanh函数即其导数图像](/img/posts/MachineLearning/tanh.jpg)
绘图代码如下
```Python
from matplotlib import pyplot as plt
import numpy as np
fig = plt.figure()
x = np.arange(-10, 10, 0.025)
plt.plot(x,(1-np.exp(-2*x))/(1+np.exp(-2*x)))
plt.title("y = (1-exp(-2x))/(1+exp(-2x))")
plt.show()
 
plt.plot(x,4*np.exp(-2*x)/(1+np.exp(-2*x))**2)
plt.title("y = 4exp(-2x)/(1+exp(-2x))^2")
plt.show()
```

- tanh函数的值域(-1,1)
- 在神经网络的应用中，tanh通常要优于Sigmoid的，因为tanh的输出在 -1~1之间，均值为0，更方便下一层网络的学习。
- 但是如果做二分类，输出层可以使用 Sigmoid，因为它可以算出属于某一类的概率。
- tanh函数也存在着梯度弥散或梯度饱和和幂运算的缺点

从tanh函数和sigmoid函数表达式中可以看出，**tanh(x)的梯度消失问题比Simgoid(x)要轻，梯度如果过早消失会导致收敛速度较慢**
- 以零为中心的影响，如果当前参数（w0, w1）的最佳优化方向是 （+d0, -d1），则根据反向传播计算公式，我们希望x0和x1符号相反，但是如果上一级神经元采用 Sigmoid 函数作为激活函数，Sigmoid不以零为中心，输出值恒为正，那么我们无法进行更快的参数更新，而是走Z字形逼近最优解。  

### ReLu函数

针对Sigmoid函数和tanh的缺点，提出ReLU函数。

线性整流函数（Rectified Linear Unit, ReLU），又称修正线性单元，是一种人工神经网络中常用的激活函数（activation function），通常指代以斜坡函数及其变种为代表的非线性函数。**解决了梯度消失现象**，,收敛速度远远大于 Sigmoid函数和 tanh函数,计算速度更快。计算方便，求导方便，计算速度非常快，只需要判断输入是否大于0。  

- **无饱和区，收敛快，计算简单**，有时候会比较脆弱，如果变量的更新太快，还没有找到最佳值，就进入小于零的分段就会使得梯度变为零，无法更新直接死掉了。

$$ {ReLu(x) = max(x,0)}$$


<center>当x>=0时，ReLu(x) = x </center>
<center>当x&lt0时， ReLu(x) = 9 </center>
<center>ReLu'(x) = 1, 当x&gt=0时</center>
<center>ReLu'(x) = 0, 当x&lt0时</center>
    


![ReLu函数即其导数图像](/img/posts/MachineLearning/ReLu.jpg)


```Python
from matplotlib import pyplot as plt
import numpy as np
fig = plt.figure()
x = np.arange(-10, 10, 0.025)
# clip(x,最小，最大)
plt.plot(x,np.clip(x,0,10e30))
plt.title("y = relu(x)=max(x,0)")
plt.show()
 
from matplotlib import pyplot as plt
plt.plot(x,x>0,"o")
plt.title("y = relu'(x)")
plt.show()
```
**ReLU函数在零点不可导**
在反向传播中，对于ReLU间断点的求导按照做到书来计算，即默认情况下默认导数为零，negative_gradient = 0

可由caffe源码可知
~/caffe/src/caffe/layers/relu_layer.cpp倒数第十行代码：
```Python
bottom_diff[i] = top_diff[i] * ((bottom_data[i] > 0)+ negative_slope * (bottom_data[i] <= 0));
```


#### ReLU的优缺点

ReLU函数就是一个取最大值函数，因为它的导数等于1或者就是0  
**因为ReLU是线性的，而sigmoid和tanh是非线性的**。相对于sigmoid和tanh激励函数，对ReLU求梯度非常简单，计算也很简单，可以非常大程度地提升随机梯度下降的收敛速度。

- 优点： 
    1. 解决了梯度消失
    2. 计算方便，秩序判断输入是否大于0
    3. 收敛速度远远大于Sigmoid函数和tanh函数，可以加速网络训练

- 缺点： 随着训练的进行，可能出现神经元死亡的情况，当有一个很大的梯度流经ReLu后，权重的更新结果可能是在此之后的任何数据点都无法激活改神经元

    1. 由于负数部分恒为零，，、当x为负时导数等于零，会导致一些神经元无法激活
    2. 输出不是zero-centered
    3. learning rate 太高，导致在训练过程中参数更新太大


#### Leaky ReLu函数

Leaky ReLU解决了ReLU会杀死一部分神经元的情况。Leaky ReLU 是给所有负值赋予一个非零斜率。Leaky ReLU 激活函数是在声学模型（2013）中首次提出。

![](/img/posts/MachineLearning/LeakyReLu.jpg)

![Leaky_ReLu函数即其导数图像](/img/posts/MachineLearning/Leaky_ReLu.jpg)


　人们为了解决 Dead ReLU Problem(ReLU的死神经问题), 提出了将 ReLU 的前半段设为 ax 而非0，通常 a = 0.01，另外一种直观的想法是基于参数的方法，即 ParmetricReLU ： f(x)=max(ax, x)，其中 a 可由方向传播算法学出来。理论上来说，Leaky ReLU 有ReLU的所有优点，外加不会有 Dead ReLU 问题，但是在实际操作当中，并没有完全证明 Leaky ReLU 总是好于 ReLU。

**Leaky  ReLU 主要是为了避免梯度消失，当神经元处于非激活状态时，允许一个非0的梯度存在**，这样不会出现梯度消失，收敛速度快。他的优缺点和ReLU类似。


### Softmax函数

[Softmax理解](https://blog.csdn.net/lz_peter/article/details/84574716?ops_request_misc=%257B%2522request%255Fid%2522%253A%2522166289193816800184112865%2522%252C%2522scm%2522%253A%252220140713.130102334..%2522%257D&request_id=166289193816800184112865&biz_id=0&utm_medium=distribute.pc_search_result.none-task-blog-2~all~top_positive~default-1-84574716-null-null.142^v47^control_1,201^v3^add_ask&utm_term=softmax&spm=1018.2226.3001.4187)

　　softmax 函数，又称为归一化指数函数。它是二分类函数 Sigmoid在多分类上的推广，目的是将多分类的结果以概率的形式展现出来。

他把**一些输入映射为0-1之间的实数，并且归一化保证和为1，因此多分类的概率之和也刚好为1。**

![Softmax解释](/img/posts/MachineLearning/Softmax01.jpg)


假设有一个数组V， $V_i$​表示V中的第i个元素，那么这个元素的softmax值为:

$$ S_i = \frac {e^i}{\sum_je^j} $$

该元素的softmax值，就是该元素的指数与所有元素指数和的比值。






## 损失函数

在机器学习任务中，大部分监督学习算法都会有一个目标函数 (Objective Function),算法对该目标函数进行优化，称为优化算法的过程。 例如在分类或者回归任务中，**使用损失函数( Loss Function )作为其目标函数对算法模型进行优化**。

在BP神经网络中，使用均方误差作为损失函数，而在实际中，常用交叉熵作为损失函数。
- 均方误差作为损失函数收敛速度慢，可能会陷入局部最优解；**交叉熵**作为损失函数的**收敛速度**比均方误差**快**，且较为容易找到函数最优解.

- 均方差损失函数：
    ![](/img/posts/MachineLearning/avg_sqar.jpg)

- Logistic 损失函数
    ![](/img/posts/MachineLearning/logistic_loss.jpg)

- 负对数似然损失函数
    ![](/img/posts/MachineLearning/neg_log_loss.jpg)

- [交叉熵损失函数](https://blog.csdn.net/tsyccnh/article/details/79163834?ops_request_misc=%257B%2522request%255Fid%2522%253A%2522166289228816782412517434%2522%252C%2522scm%2522%253A%252220140713.130102334..%2522%257D&request_id=166289228816782412517434&biz_id=0&utm_medium=distribute.pc_search_result.none-task-blog-2~all~top_positive~default-1-79163834-null-null.142^v47^control_1,201^v3^add_ask&utm_term=%E4%BA%A4%E5%8F%89%E7%86%B5&spm=1018.2226.3001.4187):Logistic损失函数和负对数似然损失函数只能处理二分类问题，对于两个分类扩展到M个分类，使用交叉熵损失函数(Cross Entropy Loss)，熵用来表示所有信息量的期望[信息量：当越不可能的事件发生了，我们获取到的信息量就越大。越可能发生的事件发生了，我们获取到的信息量就越小，信息量和事件发生的概率]，其定义如下：
    ![](/img/posts/MachineLearning/cross_entropy_loss.jpg)
    - 

- Hige:典型分类器是SVM算法因为 Hinge 损失可以用来解决间隔最大化问题。当分类模型需要硬分类结果的，例如分类结果是 0 或 1 、 -1或 1 的二分类数据， Hinge 损失是最方便的选择 。Hinge 损失函数定义如下：
    ![](/img/posts/MachineLearning/hinge_loss.jpg)

### 神经网络中常用的损失函数
[常用损失函数参考介绍](https://blog.csdn.net/weixin_44222014/article/details/103271192)
1. ReLU + MSE(均方差)
    均方误差损失函数无法处理梯度消失问题，而使用 Leak ReLU 激活函数能够减少计算时梯度消失的问题，因此在神经网络中如果需要使用均方误差损失函数，一般采用 Leak ReLU 等可以减少梯度消失的激活函数。另外，由于均方误差具有普遍性，一般作为衡量损失值的标准，因此使用均方误差作为损失函数表现既不会太好也不至于太差。

2. Sigmoid + Logistic
    Sigmoid 函数会引起梯度消失问题：根据链式求导法，Sigmoid 函数求导后由多个［0, 1］范围的数进行连乘，如其导数形式为 ，当其中一个数很小时，连成后会无限趋近于零直至最后消失。而类 Logistic 损失函数求导时，加上对数后连乘操作转化为求和操作，在一定程度上避免了梯度消失，所以我们经常可以看到 Sigmoid 激活函数＋交叉摘损失函数 的组合。

3. Softmax + Logisitc
    在数学上，Softmax 激活函数会返回输出类的互斥概率分 布，也就是能把离散的输出转换为一个同分布互斥的概率，如(0.2, 0.8)。另外，Logisitc 损失函数是基于概率的最大似然估计函数而来的，因此输出概率化能够更加方便优化算法进行求导和计算，所以我们经常可以看到输出层使xu用Softmax激活函数＋交叉熵损失函数 的组合。



## 优化方法

### 梯度下降

深度学习网络训练过程可以分成两大部分：前向计算过程与反向传播过程。前向计算过程，是指通过我们预先设定好的卷积层、池化层等等，**按照规定的网络结构一层层前向计算，得到预测的结果**。反向传播过程，是为了将设定的网络中的**众多参数**一步步调整，使得**预测结果能更加贴近真实值**。在反向传播中参数更新显得尤为重要。

参数应该是**朝着目标损失函数下降最快的方向**更新，更确切的说，要朝着**梯度方向**更新

- 随机梯度下降法：每次迭代（更新参数）只使用单个训练样本
    优点：一次迭代只需对一个样本进行计算，因此运行速度很快，还可用于在线学习
    缺点：（1）由于单个样本的随机性，实际过程中，目标损失函数值会剧烈波动，一方面，SGD 的波动使它能够跳到新的可能更好的局部最小值。另一方面，使得训练永远不会收敛，而是会**一直在最小值附近波动**。（2）一次迭代只计算一张图片，没有发挥GPU并行运算的优势，使得**整体计算的效率不高**。


- 批量梯度下降法：每次迭代更新中使用所有的训练样本
    优缺点分析：批量梯度下降算法能保证收敛到凸误差表面的全局最小值和非凸表面的局部最小值。但每迭代一次，需要用到训练集中的所有数据，如果数据量很大，那么迭代速度就会非常慢。

- 小批量梯度下降法：折中了 BGD 和 SGD 的方法，每次迭代使用 batch_size 个训练样本进行计算
    优缺点分析：因为每次迭代使用多个样本，所以 MBGD 比 SGD 收敛更稳定，也能避免 BGD 在数据集过大时迭代速度慢的问题。因此，MBGD是深度学习网络训练中经常使用的梯度下降方法。

### 动量梯度下降

动量梯度下降使得当前的参数更新方向不仅与当前的梯度有关，也受历史的加权平均梯度影响。对于梯度指向相同方向的维度，动量会积累并增加，而对于梯度改变方向的维度，动量会减少更新。这也就使得收敛速度加快，同时又不至于摆动幅度太大。

本质上，当使用动量时，如同我们将球推下山坡。球在滚下坡时积累动量，在途中变得越来越快。同样的事情发生在参数更新上：对于梯度指向相同方向的维度，动量会积累并增加，而对于梯度改变方向的维度，动量会减少更新。**结果，我们获得了更快的收敛和减少的振荡**。


### Adam优化器
Adam 是另一种参数自适应学习率的方法，相当于 RMSprop + Momentum，利用梯度的一阶矩估计和二阶矩估计动态调整每个参数的学习率

## 激活函数、损失函数、优化函数的区别

1.激活函数：将神经网络上一层的输入，经过神经网络层的**非线性变换转换**后，通过激活函数，**得到输出**。常见的激活函数包括：sigmoid, tanh, relu等。

[激活函数](https://blog.csdn.net/u013250416/article/details/80991831)

2.损失函数：度量神经网络的**输出的预测值与实际值之间的差距**的一种方式。常见的损失函数包括：最小二乘损失函数、交叉熵损失函数、回归中使用的smooth L1损失函数等。

3.优化函数：也就是如何把**损失值**从神经网络的最外层**传递到最前面**。如最基础的梯度下降算法，随机梯度下降算法，批量梯度下降算法，带动量的梯度下降算法，Adagrad，Adadelta，Adam等。

[优化函数](https://blog.csdn.net/u013250416/article/details/81090059)

CNN由**卷积层**、**池化层**、**全连接层**三部分组成。

## 卷积神经网络CNN

[卷积](https://www.zhihu.com/question/22298352/answer/91131073)

卷积网络执行的是监督训练，所以其样本集是由形如：（输入向量，理想输出向量）的向量对构成的。

- CNN的特点：
    1. 能够有效的将大数据量的图片降维成小数据量。
    2. 能够有效的保留图片特征，符合图片处理的原则。

- CNN的核心：
    1. 局部连接：通过卷积操作实现局部连接，这个局部区域的大小就是滤波器filter，避免了全连接中参数过多造成无法计算的情况。
    一般认为图像的空间联系是局部的像素联系比较密切，而距离较远的像素相关性较弱，因此，**每个神经元没必要对全局图像进行感知，只要对局部进行感知**，然后在**更高层**将**局部的信息综合起来**得到全局信息。
    - 具体实现： 网络部分连通，每个神经元只与上一层的部分神经元相连，只感知局部，而不是整幅图像。

    2. 权值(参数)共享：通过权值共享来缩减实际参数的数量，为实现多层网络提供了可能。
    在局部连接中，每个神经元的参数都是一样的，即：**同一个卷积核在图像中都是共享的。**(对于图像上这部分学到的特征也可以用到另一部分上。所以对图像上的所有位置，都能使用同样的学习特征。)
    但卷积核共享会导致提取特征不充分，因此要**通过增加多个卷积核来弥补**，学习多种特征

CNN在本质上是一种输入到输出的映射，它能够学习大量的输入与输出之间的映射关系，而不需要任何输入和输出之间的精确的数学表达式，只要用已知的模式对卷积网络加以训练，网络就具有输入输出对之间的映射能力。

### 卷积层

以图像处理为例， 一般用一个正方形卷积核，遍历图片上的每一个像素点。**图片与卷积核重合区域内**相对应的**每一个像素值**，乘**卷积核内相对应点**的**权重**，然后**求和**， 再加上偏置后，最后得到输出图片中的一个像素值。
图片分**灰度图**和**彩色图**，卷积核可以是**单个**也可以是**多个**，因此卷积操作分以下三种情况：

1. 单通道输入，单卷积核

    单通道输入即灰度图，但卷积核即卷积核个数为1。
    ![](/img/posts/MachineLearning/simple_convo.jpg)

    上面是 5x5x1 的灰度图片，1表示单通道，5x5 表示分辨率，共有 行5列个灰度值。若用一个 3x3x1 的卷积核对  此5x5x1的灰度图片进行卷积，偏置项b=1，则求卷积的计算是：(-1)x1+0x0+1x2+(-1)x5+0x4+1x2+(-1)x3+0x4+1x5+1=1（注意不要忘记加偏置 1）。


2. 多通道输入，单卷积核

    输入为彩色图片，输入的图片包含了红、绿、蓝三层数据，**卷积核的深度（通道数）应该等于输入图片的通道数**，所以使用 3x3x3的卷积核，最后一个 3 表示匹配输入图像 的 3 个通道，这样这个卷积核有三通道，**每个通道都会随机生成 9 个待优化的参数**，一共有**27 个待优化参数  w**和**一个偏置 b**。
     ![](/img/posts/MachineLearning/simple_convo.jpg)

    这里还是单个卷积核的情况，但是一个卷积核可以有多个通道。默认情况下，卷积核的通道数等于输入图片的通道数。

3. 多通道输入，多卷积核

    这是神经网络中最常见的方式，此处以3通道输入，两个卷积核为例。
    ![](/img/posts/MachineLearning/multi_convo.jpg)
    
    1. 和多通道输入，单卷积核一样,先取出一个卷积核与3通道的输入进行卷积,得到一个1通道的输出output1。同样再取出第二个卷积核进行同样的操作,得到第二个输出output2。
    2. 将相同size的output1与output2进行堆叠，就得到2通道的输出output。
    图中输入X:[1,h,w,3]指的是：输入1张高h宽w的3通道图片。
    卷积核W:[k,k,3,2]指的是：卷积核尺寸为3*3，通道数为3，个数为2。


- **卷积核的个数和卷积核的通道数**是不同的概念。每层卷积核的个数在设计网络时会给出，但是卷积核的通道数不一定会给出。**默认情况下，卷积核的通道数=输入的通道数**，因为这是进行卷积操作的必要条件。
- 卷积操作后，**输出的通道数=卷积核的个数**
- 偏置数=卷积核数


在卷积层中，可以通过调节步长参数 s 实现特征图的高宽成倍缩小，从而降低网络的参数量。

卷积层参数/卷积计算量
卷积参数 = 卷积核长度x卷积核宽度x输入通道数x输出通道数+输出通道数（偏置）
卷积计算量 = 输出数据大小x卷积核的尺度x输入通道数

例：输入：224x224x3，输出：224x244x64，卷积核：3x3

参数量 = 3x3x3x64+64
计算量 = 224x224x64x3x3x3


#### 填充 Padding

为了使卷积操作后能得到满意的输出图片尺寸，经常会使用padding对输入进行填充操作。默认在图片周围填充0。

### 池化层（Pooling-layel)

功能：主要作用是把数据降维，可以有效的避免过拟合 
池化作用如下
1. 使卷积神经网络**抽取特征**是**保证特征局部不变性。**
2. 池化操作能**降低维度**，**减少参数数量**。
3. 池化操作优化比较简单。

![](/img/posts/MachineLearning/pooling.jpg)
    
![](/img/posts/MachineLearning/pooling2.jpg)


池化层同样基于**局部相关性**的思想，通过从**局部相关**的一组元素中进行采样或信息聚合，从而得到新的元素值。通常我们用到两种池化进行下采样：
（1）最大池化(Max Pooling)，从局部相关元素集中选取最大的一个元素值。
（2）平均池化(Average Pooling)，从局部相关元素集中计算平均值并返回。

### 全连接层

功能：根据不同任务输出我们想要的结果

每个神经元与前后相邻层的每一个神经元都有连接关系,输入特征，输出预测的结果。
全连接层的参数量 = $\sum {(前层x后层+后层)}$

由于一般全连接层参数较多，实际应用中一般不会将原始图片直接喂入全连接网络。尤其现实生活中高分辨率的彩色图像，像素点更多，且为红绿蓝三通道信息。待优化的参数过多， 容易导致模型过拟合。
在实际应用中，会先对原始图像**进行卷积特征提取**，把提取到的**特征**喂给全连接网络，再让全连接网络计算出分类评估值。

### 激活函数

卷积神经网络中最常用的是ReLU。
[激活函数的选择](https://blog.csdn.net/wjinjie/article/details/104729911)


### 卷积神经网络的训练步骤

![](/img/posts/MachineLearning/convo_procedure.jpg)
    

1. 用随机数初始化**所有的卷积核参数/权重**

2. 将训练集图片作为输入，进行前向步骤，包括**卷积**、**ReLU激活**、**池化**、**全连接层的前向传播**，并计算各个类别对应的输出概率
3. 计算出书层的总误差
4. 用误差**反向传播**算法计算**误差相对于所有权重的梯度**，利用梯度下降**更新所有的卷积核参数/权重的值**

卷积核个数、卷积核尺寸、网络架构这些参数，是在 之前就已经固定的，且不会在训练过程中改变——只有**卷积核矩阵**和**神经元权重**会更新。

#### 经典网络介绍

[卷积神经网络介绍](https://blog.csdn.net/jiaoyangwm/article/details/80011656)
- 残差网络(Residual Network,ResNet)：通过非线性的卷积层增加直连边，也就是残差连接的方式来提高信息传播小笼包

- AlexNet


## 循环神经网络RNN


[循环神经网络](https://my.oschina.net/u/876354/blog/1621839)通过使用带自反馈的神经元，能够处理任意长度的序列，比前馈神经网络更符合生物神经网络结构。
RNN 跟传统神经网络最大的区别在于每次都会将前一次的输出结果，带到下一次的隐藏层中，一起训练。




## 目标检测算法-Yolo

Yolo是one-stage，一步回归到位，与two-stage相比，核心优势在于速度非常快，适合做实时检测任务，但可能相比two-stage(MaskRcnn)的检测精度效果会相对较差

考虑指标；
- map：map越高，效果越好，，不要单看精度、召回率。把所有阈值都考虑进来，以recall为横轴，precison为纵轴，作一个曲线图，map值就是所有类别的平均值
- IoU：/frac {预测值与真实值的重叠区域(交集)}{预测值与真实值的并集}，IoU越高越好
- 精度 = /frac {TP}{TP+FP}
- 召回率 = /frac {TP}{TP+FN}

在检测任务中的精度与招呼率代表：
- TP = true positive(1被正确的判断为1) FN = false Negative（1被错误的判断为0）
- FP = false positive(0被错误的判断为1) FN = false negative（0被正确的判断为0）


- 置信度(confidence)：通俗而言就是检测一个物体它是一个人的可能性有多大，需要基于置信度阈值来计算

Yolo的map比fast-Rcnn低，但速度比rcnn快

### Yolo-V1

输入： SxS的网格

预测得出得出bounding box（x,y,w,h） + confidence ==> 四个偏移量加上一个置信度
- 置信度较高才说明是一个物体，此时再去作筛选

### Yolo-v3
